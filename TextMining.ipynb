{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNGyn8ng7zEI"
      },
      "source": [
        "# Installs and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGX4Y3_Awg-H",
        "outputId": "372bbfc4-9239-4526-c72f-87ae70f4b8da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets==1.0.1 in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (1.3.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (3.7.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (0.3.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (6.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (4.64.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.1) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.1) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.1) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.1) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.8.1rc2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.1.96)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (3.7.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.0.53)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (4.64.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tweepy\n",
        "!pip install pandas\n",
        "!pip install datasets==1.0.1\n",
        "!pip install transformers==3.1.0\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip -q install gputil\n",
        "!pip -q install psutil\n",
        "!pip -q install humanize\n",
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset, load_metric\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r17d5pIH7INz"
      },
      "source": [
        "# Get Tweets From @ko8ta_fuelhash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTbXpqfv77t5"
      },
      "outputs": [],
      "source": [
        "# API Data\n",
        "\n",
        "api_key = '7M7q9koNd8oTCS6uuoefGw6Zz'\n",
        "api_key_secret = 'luU7AI3mvtWjAMXS6p5VVpZIQTnB12Vb1Y5OYApIaOniuBBP4j'\n",
        "bearer_token = 'AAAAAAAAAAAAAAAAAAAAANaoeAEAAAAApc03i6mx77DhJ1UiF6JGmCQt8Js%3D36gVdFiy72DsDuyZkDxCMgWETxmYY61LJVJJhtV4Af7IOErKCn'\n",
        "access_token = '1540910906372005890-Dl0ELvzx2lUODUt2OWwiaXMkaZTDVy'\n",
        "access_token_secret = 'VCtxjTQeObdO9brsNAnM6DvEfneKnXMs2BukdUML5bM3O'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXFPudva8e6C"
      },
      "outputs": [],
      "source": [
        "# Authentification and Get Tweets (Drop the link)\n",
        "\n",
        "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "auth.set_access_token(access_token,access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "public_tweets = api.home_timeline()\n",
        "\n",
        "data = []\n",
        "\n",
        "for status in public_tweets:\n",
        "  text = status.text\n",
        "  text_text = text.split('http')\n",
        "  data.append([text_text[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3wBz63E868e"
      },
      "source": [
        "# Make Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Au0nt2x_E8d"
      },
      "outputs": [],
      "source": [
        "# Read the Inputfile (News Cause Drastic Price Change)\n",
        "\n",
        "path_to_data = '/content/drive/MyDrive/Sampletext.csv'\n",
        "delimiter = ','\n",
        "df_input = pd.read_csv(path_to_data, delimiter=delimiter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgC3IG2h9Bz8"
      },
      "outputs": [],
      "source": [
        "# Exceptions\n",
        "\n",
        "ascii_lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "ascii_uppercase = ascii_lowercase.upper()\n",
        "\n",
        "abbr_capped = \"|\".join([\n",
        "    \"ala|ariz|ark|calif|colo|conn|del|fla|ga|ill|ind|kan|ky|la|md|mass|mich|minn|miss|mo|mont|neb|nev|okla|ore|pa|tenn|vt|va|wash|wis|wyo\", # States\n",
        "    \"u.s\",\n",
        "    \"mr|ms|mrs|msr|dr|gov|pres|sen|sens|rep|reps|prof|gen|messrs|col|sr|jf|sgt|mgr|fr|rev|jr|snr|atty|supt|hr\", # Titles\n",
        "    \"ave|blvd|st|rd|hwy\", # Streets\n",
        "    \"jan|feb|mar|apr|jun|jul|aug|sep|sept|oct|nov|dec\", # Months\n",
        "    \"|\".join(ascii_lowercase) # Initials\n",
        "]).split(\"|\")\n",
        "\n",
        "abbr_lowercase = \"etc|v|vs|viz|al|pct\"\n",
        "\n",
        "exceptions = \"U.S.|U.N.|E.U.|F.B.I.|C.I.A.\".split(\"|\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwJ-WVPI-bV7"
      },
      "outputs": [],
      "source": [
        "# Functions needed for sentence separation\n",
        "\n",
        "def is_abbreviation(dotted_word):\n",
        "    clipped = dotted_word[:-1]\n",
        "    if clipped[0] in ascii_uppercase:\n",
        "        if clipped.lower() in abbr_capped:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        if clipped in abbr_lowercase:\n",
        "            return True\n",
        "        else:\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmyALU9Q-l5f"
      },
      "outputs": [],
      "source": [
        "def is_sentence_ender(word):\n",
        "    if word in exceptions:\n",
        "        return False\n",
        "    if word[-1] in [\"?\", \"!\", \" .\", \" .\"]:\n",
        "        return True\n",
        "    if len(re.sub(r\"[^A-Z]\", \"\", word)) > 1:\n",
        "        return True\n",
        "    if word[-1] == \".\" and (not is_abbreviation(word)):\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3vKatIp-oYf"
      },
      "outputs": [],
      "source": [
        "def split_into_sentences(text):\n",
        "    potential_end_pat = re.compile(r\"\".join([\n",
        "        r\"([\\w\\.'’&\\]\\)]+[\\.\\?!])\",  # A word that ends with punctuation\n",
        "        r\"([‘’“”'\\\"\\)\\]]*)\",  # Followed by optional quote/parens/etc\n",
        "        r\"(\\s+(?![a-z\\-–—]))\",  # Followed by whitespace + non-(lowercase or dash)\n",
        "        ]),\n",
        "        re.U\n",
        "    )\n",
        "    dot_iter = re.finditer(potential_end_pat, text)\n",
        "    end_indices = [\n",
        "        (x.start() + len(x.group(1)) + len(x.group(2)))\n",
        "        for x in dot_iter\n",
        "        if is_sentence_ender(x.group(1))\n",
        "    ]\n",
        "    spans = zip([None] + end_indices, end_indices + [None])\n",
        "    sentences = [\n",
        "        text[start:end].strip() for start, end in spans\n",
        "    ]\n",
        "    return sentences "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "sWIWSMt6-sEo",
        "outputId": "aabdebaf-6ea7-4d42-bc18-128c14460f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "s\n",
            "1\n",
            "Blockchain has evolved over the past decade, growing its utility across multiple industries.\n",
            " \n",
            "But what are some of… \n",
            "117\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     0                                                  1  \\\n",
              "0  ind                                          sentence1   \n",
              "1    1  [Blockchain has evolved over the past decade, ...   \n",
              "2    2  [Blockchain has evolved over the past decade, ...   \n",
              "3    3  [Blockchain has evolved over the past decade, ...   \n",
              "4    4  [Blockchain has evolved over the past decade, ...   \n",
              "\n",
              "                                                   2      3  \n",
              "0                                          sentence2  label  \n",
              "1  A man is in critical condition after he was sh...     20  \n",
              "2  A 33-year-old man was struck in the chest in f...     20  \n",
              "3  An unknown suspect fled the scene before polic...     20  \n",
              "4  The victim was transported to King’s County Ho...     20  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3700543c-3bd9-4472-b1ed-29fa8e632f86\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ind</td>\n",
              "      <td>sentence1</td>\n",
              "      <td>sentence2</td>\n",
              "      <td>label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[Blockchain has evolved over the past decade, ...</td>\n",
              "      <td>A man is in critical condition after he was sh...</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Blockchain has evolved over the past decade, ...</td>\n",
              "      <td>A 33-year-old man was struck in the chest in f...</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[Blockchain has evolved over the past decade, ...</td>\n",
              "      <td>An unknown suspect fled the scene before polic...</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[Blockchain has evolved over the past decade, ...</td>\n",
              "      <td>The victim was transported to King’s County Ho...</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3700543c-3bd9-4472-b1ed-29fa8e632f86')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3700543c-3bd9-4472-b1ed-29fa8e632f86 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3700543c-3bd9-4472-b1ed-29fa8e632f86');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Makind dataset\n",
        "def smaller(text,pos,times):\n",
        "  if times>0:\n",
        "    for r in row_rng:\n",
        "      sentences = split_into_sentences(df_input.values[r,1])\n",
        "      rate = df_input.values[r,2]\n",
        "      for i in range(0,len(sentences)):\n",
        "        insert = [pos,text[times-1],sentences[i],rate]\n",
        "        sent_list.append(insert)\n",
        "        pos = pos+1\n",
        "    times -= 1\n",
        "    smaller(text,pos,times)\n",
        "\n",
        "sent_list = [['ind','sentence1','sentence2','label']]\n",
        "rows = len(df_input)\n",
        "row_rng = range(0,rows)\n",
        "smaller(data,1,len(data))\n",
        "print(len('kohta'))\n",
        "for i in range(0,2):\n",
        "  print(sent_list[i][1][0])\n",
        "  print(len(sent_list[i][1][0]))\n",
        "df_test = pd.DataFrame(sent_list)\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L13BzbMYAEjB"
      },
      "source": [
        "# ALBERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9I1_Jb4AI81"
      },
      "outputs": [],
      "source": [
        "# Initial Setting\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "ixBcDh4cFyQS",
        "outputId": "73d50a66-73d1-4788-c3c0-23a3973e398c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-2fe14e35da77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mGPUs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetGPUs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPUs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprintm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# GPU Check\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "\n",
        "#gpu = GPUs[100]\n",
        "\n",
        "#def printm():\n",
        "# process = psutil.Process(os.getpid())\n",
        "# print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "# print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "#printm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHLJkxAvGD0f"
      },
      "outputs": [],
      "source": [
        "# GPU Kill (If Needed)\n",
        "\n",
        "# if gpu usage is not equal to 0%, you can kill the other tasks by this command.\n",
        "# however, this will break the whole code if you run, so please comment out if it's not neccesary\n",
        "\n",
        "#!kill -9 -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpUHZEnvGJX_"
      },
      "outputs": [],
      "source": [
        "# Load the Dataset\n",
        "# the original validation data is used as test data\n",
        "# bc the test lables are not available\n",
        "\n",
        "dataset = load_dataset('glue','mrpc')\n",
        "\n",
        "#split the original training data for validation\n",
        "split = dataset['train'].train_test_split(test_size=0.1, seed=1)\n",
        "\n",
        "#90% of the original training data\n",
        "train = split['train']\n",
        "\n",
        "#10% of the original training data\n",
        "val = split['test']\n",
        "\n",
        "df_train = pd.DataFrame(train)\n",
        "df_val = pd.DataFrame(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpnwQLIoHADL"
      },
      "outputs": [],
      "source": [
        "#checking if the dataset is correctly loaded\n",
        "\n",
        "#it will print out the size of the matrix.\n",
        "#the first number means row, and the second does column\n",
        "#the second one should be 3 or 4\n",
        "\n",
        "#print(df_train.shape)\n",
        "#print(df_val.shape)\n",
        "#print(df_test.shape)\n",
        "\n",
        "#if you want to see the actual data, you can use\n",
        "\n",
        "#df_train.head()\n",
        "#df_val.head()\n",
        "#df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPcOWpyZHDvp"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, maxlen, with_labels=True, bert_model='albert-base-v2'):\n",
        "\n",
        "        self.data = data  # pandas dataframe\n",
        "        #Initialize the tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  \n",
        "\n",
        "        self.maxlen = maxlen\n",
        "        self.with_labels = with_labels \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
        "        sent1 = str(self.data.loc[index, 'sentence1'])\n",
        "        sent2 = str(self.data.loc[index, 'sentence2'])\n",
        "\n",
        "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
        "        encoded_pair = self.tokenizer(sent1, sent2, \n",
        "                                      padding='max_length',  # Pad to max_length\n",
        "                                      truncation=True,  # Truncate to max_length\n",
        "                                      max_length=self.maxlen,  \n",
        "                                      return_tensors='pt')  # Return torch.Tensor objects\n",
        "        \n",
        "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
        "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
        "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
        "\n",
        "        if self.with_labels:  # True if the dataset has labels\n",
        "            label = self.data.loc[index, 'label']\n",
        "            return token_ids, attn_masks, token_type_ids, label  \n",
        "        else:\n",
        "            return token_ids, attn_masks, token_type_ids "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCaNUYmJHIag"
      },
      "outputs": [],
      "source": [
        "class SentencePairClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\n",
        "        super(SentencePairClassifier, self).__init__()\n",
        "        #  Instantiating BERT-based model object\n",
        "        self.bert_layer = AutoModel.from_pretrained(bert_model)\n",
        "\n",
        "        #  Fix the hidden-state size of the encoder outputs (If you want to add other pre-trained models here, search for the encoder output size)\n",
        "        if bert_model == \"albert-base-v2\":  # 12M parameters\n",
        "            hidden_size = 768\n",
        "        elif bert_model == \"albert-large-v2\":  # 18M parameters\n",
        "            hidden_size = 1024\n",
        "        elif bert_model == \"albert-xlarge-v2\":  # 60M parameters\n",
        "            hidden_size = 2048\n",
        "        elif bert_model == \"albert-xxlarge-v2\":  # 235M parameters\n",
        "            hidden_size = 4096\n",
        "        elif bert_model == \"bert-base-uncased\": # 110M parameters\n",
        "            hidden_size = 768\n",
        "\n",
        "        # Freeze bert layers and only train the classification layer weights\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # Classification layer\n",
        "        self.cls_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    @autocast()  # run in mixed precision\n",
        "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -input_ids : Tensor  containing token ids\n",
        "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
        "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
        "        '''\n",
        "\n",
        "        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n",
        "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
        "\n",
        "        # Feeding to the classifier layer the last layer hidden-state of the [CLS] token further processed by a\n",
        "        # Linear Layer and a Tanh activation. The Linear layer weights were trained from the sentence order prediction (ALBERT) or next sentence prediction (BERT)\n",
        "        # objective during pre-training.\n",
        "        logits = self.cls_layer(self.dropout(pooler_output))\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stt1vHLyHOQs"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "\n",
        "def evaluate_loss(net, device, criterion, dataloader):\n",
        "    net.eval()\n",
        "\n",
        "    mean_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\n",
        "            seq, attn_masks, token_type_ids, labels = \\\n",
        "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
        "            logits = net(seq, attn_masks, token_type_ids)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            count += 1\n",
        "\n",
        "    return mean_loss / count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV6gFlwgHP26"
      },
      "outputs": [],
      "source": [
        "!mkdir models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt0LefEQHTsP"
      },
      "outputs": [],
      "source": [
        "def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n",
        "\n",
        "    best_loss = np.Inf\n",
        "    best_ep = 1\n",
        "    nb_iterations = len(train_loader)\n",
        "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
        "    iters = []\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):\n",
        "\n",
        "            # Converting to cuda tensors\n",
        "            seq, attn_masks, token_type_ids, labels = \\\n",
        "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
        "    \n",
        "            # Enables autocasting for the forward pass (model + loss)\n",
        "            with autocast():\n",
        "                # Obtaining the logits from the model\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "\n",
        "                # Computing loss\n",
        "                loss = criterion(logits.squeeze(-1), labels.float())\n",
        "                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n",
        "\n",
        "            # Backpropagating the gradients\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (it + 1) % iters_to_accumulate == 0:\n",
        "                # Optimization step\n",
        "                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n",
        "                # otherwise, opti.step() is skipped.\n",
        "                scaler.step(opti)\n",
        "                # Updates the scale for next iteration.\n",
        "                scaler.update()\n",
        "                # Adjust the learning rate based on the number of iterations.\n",
        "                lr_scheduler.step()\n",
        "                # Clear gradients\n",
        "                opti.zero_grad()\n",
        "\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (it + 1) % print_every == 0:  # Print training loss information\n",
        "                print()\n",
        "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
        "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
        "\n",
        "                running_loss = 0.0\n",
        "\n",
        "\n",
        "        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
        "        print()\n",
        "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
        "            print()\n",
        "            net_copy = copy.deepcopy(net)  # save a copy of the model\n",
        "            best_loss = val_loss\n",
        "            best_ep = ep + 1\n",
        "\n",
        "    # Saving the model\n",
        "    path_to_model='models/{}_lr_{}_val_loss_{}_ep_{}.pt'.format(bert_model, lr, round(best_loss, 5), best_ep)\n",
        "    torch.save(net_copy.state_dict(), path_to_model)\n",
        "    print(\"The model has been saved in {}\".format(path_to_model))\n",
        "    trained_model_path = format(path_to_model)\n",
        "\n",
        "    del loss\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bywx5oh8Hihf"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBo1tNviHtDy"
      },
      "outputs": [],
      "source": [
        "bert_model = \"albert-xxlarge-v2\"  # 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2', 'bert-base-uncased', ...\n",
        "freeze_bert = False  # if True, freeze the encoder weights and only update the classification layer weights\n",
        "maxlen = 128  # maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\n",
        "bs = 10  # batch size, initially 16\n",
        "iters_to_accumulate = 2  # the gradient accumulation adds gradients over an effective batch of size : bs * iters_to_accumulate. If set to \"1\", you get the usual batch size\n",
        "lr = 2e-5  # learning rate\n",
        "epochs = 10  # number of training epochs\n",
        "trained_model_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp4a4H7yHyPS"
      },
      "outputs": [],
      "source": [
        "# If you encounter a CUDA out of memory error: \n",
        "# - uncomment the kill command, run the \"kill\" command (and comment it)\n",
        "# - reduce the batch size\n",
        "# - then run all cells from the begining \n",
        "\n",
        "# If you get an ugly print of tqdm (all iterations are showed), follow the above first and last steps\n",
        "\n",
        "#printm()\n",
        "#!kill -9 -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-knMNrK8IZjh"
      },
      "outputs": [],
      "source": [
        "from torch.cuda import device_count\n",
        "#  Set all seeds to make reproducible results\n",
        "set_seed(1)\n",
        "\n",
        "# Creating instances of training and validation set\n",
        "print(\"Reading training data...\")\n",
        "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
        "print(\"Reading validation data...\")\n",
        "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
        "# Creating instances of training and validation dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size=bs, num_workers=5)\n",
        "val_loader = DataLoader(val_set, batch_size=bs, num_workers=5)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n",
        "\n",
        "if torch.cuda.device_count() > 1:  # if multiple GPUs\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    net = nn.DataParallel(net)\n",
        "if torch.cuda.device_count() == 1:\n",
        "  print(\"One gpu\")\n",
        "else:\n",
        "  print(\"no GPU\")\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\n",
        "num_warmup_steps = 0 # The number of steps for the warmup phase.\n",
        "num_training_steps = epochs * len(train_loader)  # The total number of training steps\n",
        "t_total = (len(train_loader) // iters_to_accumulate) * epochs  # Necessary to take into account Gradient accumulation\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWWXOPbUIigR"
      },
      "outputs": [],
      "source": [
        "!mkdir results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPaTzc4ZIk6Q"
      },
      "outputs": [],
      "source": [
        "def get_probs_from_logits(logits):\n",
        "    \"\"\"\n",
        "    Converts a tensor of logits into an array of probabilities by applying the sigmoid function\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    return probs.detach().cpu().numpy()\n",
        "\n",
        "def test_prediction(net, device, dataloader, with_labels=False, result_file=\"results/output.txt\"):\n",
        "    \"\"\"\n",
        "    Predict the probabilities on a dataset with or without labels and print the result in a file\n",
        "    \"\"\"\n",
        "    net.eval()\n",
        "    w = open(result_file, 'w')\n",
        "    probs_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if with_labels:\n",
        "            for seq, attn_masks, token_type_ids, _ in tqdm(dataloader):\n",
        "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "                probs_all += probs.tolist()\n",
        "        else:\n",
        "            for seq, attn_masks, token_type_ids in tqdm(dataloader):\n",
        "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "                probs_all += probs.tolist()\n",
        "\n",
        "    w.writelines(str(prob)+'\\n' for prob in probs_all)\n",
        "    w.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5_lXexrJX6S"
      },
      "outputs": [],
      "source": [
        "path_to_model = '/content/' + trained_model_path  \n",
        "# path_to_model = '/content/models/...'  # You can add here your trained model\n",
        "\n",
        "path_to_output_file = '/content/results/output.txt'\n",
        "\n",
        "print(\"Reading test data...\")\n",
        "test_set = CustomDataset(df_test, maxlen, bert_model)\n",
        "test_loader = DataLoader(test_set, batch_size=bs, num_workers=5)\n",
        "\n",
        "\n",
        "model = SentencePairClassifier(bert_model)\n",
        "if torch.cuda.device_count() > 1:  # if multiple GPUs\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "print()\n",
        "print(\"Loading the weights of the model...\")\n",
        "model.load_state_dict(torch.load(path_to_model))\n",
        "model.to(device)\n",
        "\n",
        "print(\"Predicting on test data...\")\n",
        "# set the with_labels parameter to False if your want to get predictions on a dataset without labels\n",
        "test_prediction(net=model, device=device, dataloader=test_loader, with_labels=True, result_file=path_to_output_file)\n",
        "print()\n",
        "print(\"Predictions are available in : {}\".format(path_to_output_file))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "TextMining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XUJmJjnHbpsTdXOH_WcinAc9CojKienM",
      "authorship_tag": "ABX9TyMhgi3diov3ooMadP22++Rw"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}